{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa #get mfcc\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "import math\n",
    "import os\n",
    "from python_speech_features import mfcc\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore') #Ignore raising exception might be useful somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path):\n",
    "    samplerate, signal = wavfile.read(file_path) #read wav file\n",
    "    mfcc_feature = mfcc(signal, samplerate, numcep=13, nfft=2048,\n",
    "                                        winlen = 0.025, winstep=0.01,winfunc=np.hamming) \n",
    "    #n_fft = 1024 cause error trucated increase to 2048 to avoid\n",
    "    # delta feature\n",
    "    return mfcc_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param1:path of wave file\n",
    "# @param2:the word\n",
    "# return dataset\n",
    "def get_data(path):\n",
    "    # push all file from path into the list\n",
    "    files = os.listdir(path)\n",
    "    mfcc_data = []  # contain mfcc feature\n",
    "    dataset = {}\n",
    "    for f in files:  ##read every single .wav file and push feature into lis\n",
    "        if f.endswith(\".wav\"):\n",
    "            mfcc_data.append(extract_mfcc(os.path.join(path,f)))  # must send the whole path\n",
    "    # train test split with ideal number of test data\n",
    "    train, test = train_test_split(mfcc_data, test_size=0.3)\n",
    "    dataset = {'trainset': train, 'testset': test}\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_test_micro_data(path):\n",
    "    # push all file from path into the list\n",
    "    files = os.listdir(path)\n",
    "    mfcc_data = []  # contain mfcc feature\n",
    "    for f in files:  ##read every single .wav file and push feature into lis\n",
    "        if f.endswith(\".wav\"):\n",
    "            mfcc_data.append(extract_mfcc(os.path.join(path,f)))  # must send the whole path\n",
    "    # train test split with ideal number of test data\n",
    "    return mfcc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param1: dataset\n",
    "#param2: n_componenet (number of state)\n",
    "#param3: g_n_mix (gaussian component)\n",
    "def train_GMMHMM(train_set, n_component, g_n_mix):\n",
    "    model = hmm.GMMHMM(n_components=n_component, n_mix=g_n_mix,covariance_type='diag',\n",
    "                       n_iter=10)\n",
    "    #reshape data for model to train \n",
    "    length = []\n",
    "    flat_data = []\n",
    "    for m in train_set:\n",
    "        length.append(len(m))\n",
    "        flat_data += m.flatten().tolist() \n",
    "#     print((flat_data))\n",
    "    flat_data = np.array(flat_data).reshape(-1,13) #numcep = 13 \n",
    "    # fit dat \n",
    "    model.fit(flat_data,lengths = length) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5 63 67\n"
     ]
    }
   ],
   "source": [
    "###MAIN\n",
    "##########get data\n",
    "##và\n",
    "path_va = \"dataset/va/\"\n",
    "##không\n",
    "path_khong = \"dataset/khong/\"\n",
    "##của\n",
    "path_cua = \"dataset/cua/\"\n",
    "##người\n",
    "path_nguoi = \"dataset/nguoi/\"\n",
    "##quốc tế\n",
    "path_quoc_te = \"dataset/quoc_te/\"\n",
    "###get data deature\n",
    "dataset_va = get_data(path_va)\n",
    "dataset_khong = get_data(path_khong)\n",
    "dataset_cua = get_data(path_cua)\n",
    "dataset_nguoi = get_data(path_nguoi)\n",
    "dataset_quoc_te = get_data(path_quoc_te)\n",
    "###get train set and test set\n",
    "data = {}\n",
    "data[\"va\"] = dataset_va\n",
    "data[\"khong\"] = dataset_khong\n",
    "data[\"cua\"] = dataset_cua\n",
    "data[\"nguoi\"] = dataset_nguoi\n",
    "data[\"quoc_te\"] = dataset_quoc_te\n",
    "\n",
    "print(len(data))\n",
    "print(len(data), len(data[\"va\"][\"trainset\"]),len(data[\"khong\"][\"trainset\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "########train HMM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "####for và n_component = 6, n_mix = 2\n",
    "#n_mix = 2 , base on gender voice distribution\n",
    "gmmhmms = {}\n",
    "model_va = train_GMMHMM(data[\"va\"][\"trainset\"],6,2) # 2 am vi -> 6 component\n",
    "model_khong = train_GMMHMM(data[\"khong\"][\"trainset\"],6,2)\n",
    "model_cua = train_GMMHMM(data[\"cua\"][\"trainset\"],6,2)\n",
    "model_nguoi = train_GMMHMM(data[\"nguoi\"][\"trainset\"],6,2)\n",
    "model_quoc_te = train_GMMHMM(data[\"quoc_te\"][\"trainset\"],12,2) # 3 am vi -> 12 component\n",
    "gmmhmms[\"va\"] = model_va\n",
    "gmmhmms[\"khong\"] = model_khong\n",
    "gmmhmms[\"cua\"] = model_cua\n",
    "gmmhmms[\"nguoi\"] = model_nguoi\n",
    "gmmhmms[\"quoc_te\"] = model_quoc_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
      "       covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5],\n",
      "        [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5]],\n",
      "\n",
      "       [[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5],\n",
      "        [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5]],\n",
      "\n",
      "       [[-1.5, -...\n",
      "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]),\n",
      "       means_weight=array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]),\n",
      "       min_covar=0.001, n_components=6, n_iter=10, n_mix=2, params='stmcw',\n",
      "       random_state=None, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
      "       verbose=False,\n",
      "       weights_prior=array([[1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.]]))\n",
      "GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
      "       covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5],\n",
      "        [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5]],\n",
      "\n",
      "       [[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5],\n",
      "        [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5]],\n",
      "\n",
      "       [[-1.5, -...\n",
      "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]),\n",
      "       means_weight=array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]),\n",
      "       min_covar=0.001, n_components=6, n_iter=10, n_mix=2, params='stmcw',\n",
      "       random_state=None, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
      "       verbose=False,\n",
      "       weights_prior=array([[1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.]]))\n",
      "GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
      "       covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5],\n",
      "        [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5]],\n",
      "\n",
      "       [[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5],\n",
      "        [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5]],\n",
      "\n",
      "       [[-1.5, -...\n",
      "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]),\n",
      "       means_weight=array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]),\n",
      "       min_covar=0.001, n_components=6, n_iter=10, n_mix=2, params='stmcw',\n",
      "       random_state=None, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
      "       verbose=False,\n",
      "       weights_prior=array([[1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.]]))\n",
      "GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
      "       covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5],\n",
      "        [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5]],\n",
      "\n",
      "       [[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5],\n",
      "        [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5]],\n",
      "\n",
      "       [[-1.5, -...\n",
      "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]),\n",
      "       means_weight=array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]),\n",
      "       min_covar=0.001, n_components=6, n_iter=10, n_mix=2, params='stmcw',\n",
      "       random_state=None, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
      "       verbose=False,\n",
      "       weights_prior=array([[1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.]]))\n",
      "GMMHMM(algorithm='viterbi', covariance_type='diag',\n",
      "       covars_prior=array([[[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5],\n",
      "        [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5]],\n",
      "\n",
      "       [[-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5],\n",
      "        [-1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5, -1.5,\n",
      "         -1.5, -1.5, -1.5]],\n",
      "\n",
      "       [[-1.5, -...\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]),\n",
      "       min_covar=0.001, n_components=12, n_iter=10, n_mix=2, params='stmcw',\n",
      "       random_state=None, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
      "       verbose=False,\n",
      "       weights_prior=array([[1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "print(len(gmmhmms))\n",
    "print(gmmhmms[\"va\"])\n",
    "print(gmmhmms[\"khong\"])\n",
    "print(gmmhmms[\"cua\"])\n",
    "print(gmmhmms[\"nguoi\"])\n",
    "print(gmmhmms[\"quoc_te\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "###saving model using pickle\n",
    "filename = \"hmmmodel.sav\"\n",
    "pickle.dump(gmmhmms, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and calulate accuracy\n",
    "gmmhmms1 = pickle.load(open(\"hmmmodel.sav\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(gmmhmms1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "####getting score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict function\n",
    "#it iterate through all model \n",
    "#and check the test belong to which one of the model \n",
    "#maximum socre -> test file belong to the model go with it \n",
    "def predict(fileData):\n",
    "    logOddsToKey = {}\n",
    "    for key in gmmhmms1:\n",
    "        ghmm = gmmhmms1[key] #get the model in models dictionary\n",
    "        logOdds = gmmhmms1[key].score_samples(fileData)[0] #calculate score\n",
    "        logOddsToKey[logOdds] = key #\n",
    "    return logOddsToKey[max(logOddsToKey.keys())] #return the equivalent label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledTestData = []\n",
    "for word in data:\n",
    "    for fileData in data[word]['testset']:\n",
    "        labeledTestData.append((fileData, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "va va\n",
      "va va\n",
      "quoc_te va\n",
      "quoc_te va\n",
      "cua va\n",
      "va va\n",
      "va va\n",
      "cua va\n",
      "va va\n",
      "va va\n",
      "quoc_te va\n",
      "cua va\n",
      "nguoi va\n",
      "quoc_te va\n",
      "va va\n",
      "va va\n",
      "nguoi va\n",
      "cua va\n",
      "khong va\n",
      "cua va\n",
      "va va\n",
      "va va\n",
      "cua va\n",
      "cua va\n",
      "cua va\n",
      "cua va\n",
      "khong va\n",
      "va va\n",
      "va va\n",
      "cua va\n",
      "cua va\n",
      "khong va\n",
      "cua va\n",
      "khong khong\n",
      "khong khong\n",
      "cua khong\n",
      "quoc_te khong\n",
      "cua khong\n",
      "quoc_te khong\n",
      "quoc_te khong\n",
      "quoc_te khong\n",
      "cua khong\n",
      "quoc_te khong\n",
      "quoc_te khong\n",
      "cua khong\n",
      "khong khong\n",
      "khong khong\n",
      "quoc_te khong\n",
      "cua khong\n",
      "cua khong\n",
      "quoc_te khong\n",
      "khong khong\n",
      "khong khong\n",
      "cua khong\n",
      "khong khong\n",
      "khong khong\n",
      "khong khong\n",
      "khong khong\n",
      "cua khong\n",
      "cua khong\n",
      "quoc_te khong\n",
      "cua khong\n",
      "cua khong\n",
      "quoc_te khong\n",
      "nguoi khong\n",
      "cua khong\n",
      "quoc_te khong\n",
      "quoc_te cua\n",
      "quoc_te cua\n",
      "khong cua\n",
      "quoc_te cua\n",
      "cua cua\n",
      "cua cua\n",
      "quoc_te cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "quoc_te cua\n",
      "nguoi cua\n",
      "cua cua\n",
      "cua cua\n",
      "quoc_te cua\n",
      "khong cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "khong cua\n",
      "cua cua\n",
      "cua cua\n",
      "nguoi cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "quoc_te cua\n",
      "cua cua\n",
      "cua cua\n",
      "nguoi cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "cua cua\n",
      "nguoi nguoi\n",
      "nguoi nguoi\n",
      "cua nguoi\n",
      "nguoi nguoi\n",
      "nguoi nguoi\n",
      "nguoi nguoi\n",
      "nguoi nguoi\n",
      "cua nguoi\n",
      "khong nguoi\n",
      "cua nguoi\n",
      "cua nguoi\n",
      "quoc_te nguoi\n",
      "nguoi nguoi\n",
      "nguoi nguoi\n",
      "quoc_te nguoi\n",
      "cua nguoi\n",
      "cua nguoi\n",
      "nguoi nguoi\n",
      "nguoi nguoi\n",
      "quoc_te nguoi\n",
      "nguoi nguoi\n",
      "nguoi nguoi\n",
      "quoc_te nguoi\n",
      "quoc_te nguoi\n",
      "quoc_te nguoi\n",
      "cua nguoi\n",
      "khong nguoi\n",
      "cua nguoi\n",
      "cua nguoi\n",
      "khong nguoi\n",
      "nguoi nguoi\n",
      "nguoi nguoi\n",
      "quoc_te nguoi\n",
      "khong nguoi\n",
      "cua nguoi\n",
      "cua nguoi\n",
      "khong nguoi\n",
      "cua nguoi\n",
      "cua quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "cua quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "cua quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "cua quoc_te\n",
      "cua quoc_te\n",
      "nguoi quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "quoc_te quoc_te\n",
      "0.3669064748201439\n"
     ]
    }
   ],
   "source": [
    "####this code iterate through test set\n",
    "### see if it belong to the label word in trained set\n",
    "###if true increase success variance\n",
    "success = 0\n",
    "for test in labeledTestData:\n",
    "    prediction = predict(test[0])\n",
    "    print(prediction + \" \" + test[1])\n",
    "    if prediction == test[1]:\n",
    "        print(prediction + \" \" + test[1])\n",
    "        success += 1\n",
    "\n",
    "accuracy = success / len(labeledTestData)\n",
    "print(accuracy)\n",
    "#accuracy on the whole test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "va cua\n",
      "\n",
      "va cua\n",
      "\n",
      "va khong\n",
      "\n",
      "va khong\n",
      "\n",
      "va nguoi\n",
      "\n",
      "nguoi nguoi\n",
      "\n",
      "cua quoc_te\n",
      "\n",
      "cua quoc_te\n",
      "\n",
      "va va\n",
      "\n",
      "nguoi va\n",
      "\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "###test from micro to predict word\n",
    "#TODO: write to get all the test from micro phone\n",
    "cate = [\"cua\",\"khong\",\"nguoi\",\"quoc_te\",\"va\"]\n",
    "success_on_record = 0\n",
    "leng_of_data = 0\n",
    "for c in cate:\n",
    "    s = \"du-lieu/\"\n",
    "    s = s + c\n",
    "    mffc_data = get_test_micro_data(s)\n",
    "    for test in mffc_data:\n",
    "        leng_of_data += 1\n",
    "        result = predict(test)\n",
    "        print(result + \" \" + c + \"\\n\")\n",
    "        if result == c:\n",
    "            success_on_record += 1\n",
    "print(success_on_record/leng_of_data)\n",
    "\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
